{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 직접 구현해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"원숭이, 바나나, 사과\"로 원-핫 인코딩을 한다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 대상 단어들을 담은 리스트\n",
    "word_ls = ['원숭이', '바나나', '사과', '개', '고양이']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def on_hot_encode(word_ls):\n",
    "    \n",
    "    # 고유 단어와 인덱스를 매칭시켜주는 사전 생성\n",
    "    word2id_dic = defaultdict(lambda:len(word2id_dic))\n",
    "    \n",
    "    # {단어 : 인덱스} 사전 구축\n",
    "    for word in word_ls:\n",
    "        word2id_dic[word]\n",
    "        \n",
    "    n_uniue_words = len(word2id_dic)  # 고유한 단어의 갯수\n",
    "    one_hot_vectors = np.zeros((len(word_ls), n_uniue_words))  # 원핫-벡터를 만들기 위해 비어있는 벡터 생성\n",
    "    \n",
    "    for i, word in enumerate(word_ls):\n",
    "        index = word2id_dic[word]  # 해당 단어의 고유 인덱스\n",
    "        one_hot_vectors[i, index] = 1  # 해당 단어의 고유 인덱스에만 1을 더해줌\n",
    "        \n",
    "    return one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vectors = on_hot_encode(word_ls)\n",
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 \"코끼리\"라는 단어가 추가된다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ls.append('코끼리')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vectors = on_hot_encode(word_ls)\n",
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 sklearn 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['원숭이' '바나나' '사과' '개' '고양이' '코끼리']\n",
      "[4 2 3 0 1 5]\n",
      "[[4]\n",
      " [2]\n",
      " [3]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "['원숭이']\n"
     ]
    }
   ],
   "source": [
    "# sklearn을 활용한 one-hot encoding\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 예제 데이터 배열\n",
    "values = array(word_ls)\n",
    "print(values)\n",
    "\n",
    "# 문자열에 숫자를 붙임\n",
    "label_enc = LabelEncoder()\n",
    "int_enc = label_enc.fit_transform(values)\n",
    "print(int_enc)\n",
    "\n",
    "# binary encode\n",
    "onehot_enc = OneHotEncoder(sparse=False)\n",
    "int_enc = int_enc.reshape(len(int_enc), 1)  # n:1 matrix로 변환\n",
    "print(int_enc)\n",
    "onehot_enc = onehot_enc.fit_transform(int_enc)\n",
    "print(onehot_enc)\n",
    "\n",
    "# one-hot encoding의 첫번째 배열을 값을 역으로 산출\n",
    "inverted = label_enc.inverse_transform([argmax(onehot_enc[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 밀집 벡터 (Denses Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 유사도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 유클리디언 거리(Euclidean distance)\n",
    "두 벡터사이의 직선 거리. 피타고라스 정리를 생각하면 이해하기 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dic = {\n",
    "    '사과' : [1.0, 0.5],\n",
    "    '바나나' : [0.9, 1.2],\n",
    "    '원숭이' : [0.5, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def euclidean_dist(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "# 사과와 바나나의 코사인 유사도\n",
    "euclidean_dist(word_embedding_dic['사과'], word_embedding_dic['바나나'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 자카드 유사도(Jaccard index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'원숭이는', '싫어합니다.', '대부분', '코주부', '바나나를', '좋아합니다.'}\n",
      "{'원숭이는', '바나나를'}\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "s1 = '대부분 원숭이는 바나나를 좋아합니다.'\n",
    "s2 = '코주부 원숭이는 바나나를 싫어합니다.'\n",
    "\n",
    "# 토큰화를 수행합니다.\n",
    "token_s1 = s1.split()\n",
    "token_s2 = s2.split()\n",
    "\n",
    "union = set(token_s1).union(set(token_s2))\n",
    "print(union)\n",
    "\n",
    "intersection = set(token_s1).intersection(set(token_s2))\n",
    "print(intersection)\n",
    "\n",
    "print(len(intersection)/len(union)) # 2를 6로 나눔."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 코사인 유사도(Consine Similarity)\n",
    "- 두 벡터간의 유사도를 측정하는 방법 중 하나\n",
    "- 두 벡터 사이의 코사인을 측정\n",
    "- 0도 = 1,  90도 = 0,  180도 = -1  ==> 1에 가까울수록 유사도가 높음(각도가 작을수록)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    # x와 y, 두 벡터의 코사인 유사도를 계산하는 함수\n",
    "    nominator = np.dot(x, y)  # 분자\n",
    "    denominator = np.linalg.norm(x)*np.linalg.norm(y)\n",
    "    return nominator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8944271909999159\n",
      "0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "# 사과와 바나나의 코사인 유사도\n",
    "print(cosine_similarity(word_embedding_dic['사과'], word_embedding_dic['바나나']))\n",
    "print(euclidean_dist(word_embedding_dic['사과'], word_embedding_dic['바나나']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TF-IDF를 활용한 단어 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 직접 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"The cat sat on my face I hate a cat\"\n",
    "d2 = \"The dog sat on my bed I love a dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs :  [['The', 'cat', 'sat', 'on', 'my', 'face', 'I', 'hate', 'a', 'cat'], ['The', 'dog', 'sat', 'on', 'my', 'bed', 'I', 'love', 'a', 'dog']]\n",
      "================== document [0] =======================\n",
      "[('The', 0.0), ('cat', 0.13862943611198905), ('sat', 0.0), ('on', 0.0), ('my', 0.0), ('face', 0.06931471805599453), ('I', 0.0), ('hate', 0.06931471805599453), ('a', 0.0), ('cat', 0.13862943611198905)]\n",
      "================== document [1] =======================\n",
      "[('The', 0.0), ('dog', 0.13862943611198905), ('sat', 0.0), ('on', 0.0), ('my', 0.0), ('bed', 0.06931471805599453), ('I', 0.0), ('love', 0.06931471805599453), ('a', 0.0), ('dog', 0.13862943611198905)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# document 내 토큰이 등장한 빈도수 계산\n",
    "def f(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "# tf 계산\n",
    "def tf(t, d):\n",
    "    return f(t, d)/len(d)\n",
    "\n",
    "# idf 계산\n",
    "def idf(t, D):\n",
    "    numerator = len(D)\n",
    "    denominator = len([True for d in D if t in d])\n",
    "    return np.log(numerator/denominator)\n",
    "\n",
    "# tf-idf 계산\n",
    "def tfidf(t, d, D):\n",
    "    return tf(t, d) * idf(t, D)\n",
    "\n",
    "# 공백을 기준으로 토큰화\n",
    "def tokenizer(d):\n",
    "    return d.split()\n",
    "\n",
    "# tfidf 계산\n",
    "def tfidfScorer(D):\n",
    "    docs = [tokenizer(d) for d in D]\n",
    "    print(\"docs : \", docs)\n",
    "    result = []\n",
    "    for d in docs: \n",
    "        result.append([(t, tfidf(t, d, docs)) for t in d])\n",
    "    return result\n",
    "\n",
    "corpus = [d1, d2]\n",
    "\n",
    "for i, doc in enumerate(tfidfScorer(corpus)):\n",
    "    print(\"================== document [%d] =======================\" % i)\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 sklearn 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 1 1 0 1 1 1 1]\n",
      " [1 0 2 0 0 1 1 1 1 1]]\n",
      "{'the': 9, 'cat': 1, 'sat': 8, 'on': 7, 'my': 6, 'face': 3, 'hate': 4, 'dog': 2, 'bed': 0, 'love': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "d1 = \"The cat sat on my face I hate a cat\"\n",
    "d2 = \"The dog sat on my bed I love a dog\"\n",
    "corpus = [d1, d2]\n",
    "count_vect = CountVectorizer()\n",
    "countv = count_vect.fit_transform(corpus)\n",
    "print(countv.toarray())  # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(count_vect.vocabulary_)  # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.70600557 0.         0.35300279 0.35300279 0.\n",
      "  0.25116439 0.25116439 0.25116439 0.25116439]\n",
      " [0.35300279 0.         0.70600557 0.         0.         0.35300279\n",
      "  0.25116439 0.25116439 0.25116439 0.25116439]]\n",
      "{'the': 9, 'cat': 1, 'sat': 8, 'on': 7, 'my': 6, 'face': 3, 'hate': 4, 'dog': 2, 'bed': 0, 'love': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "d1 = \"The cat sat on my face I hate a cat\"\n",
    "d2 = \"The dog sat on my bed I love a dog\"\n",
    "corpus = [d1, d2]\n",
    "tfidf_vect = TfidfVectorizer().fit(corpus)\n",
    "tfidfv = tfidf_vect.transform(corpus)\n",
    "print(tfidfv.toarray())\n",
    "print(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 gensim 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "\n",
    "d1 = \"The cat sat on my face I hate a cat\"\n",
    "d2 = \"The dog sat on my bed I love a dog\"\n",
    "corpus = [d1, d2]\n",
    "\n",
    "doc_ls = [doc.split() for doc in corpus]\n",
    "id2word = corpora.Dictionary(doc_ls)  # fit dictionary\n",
    "corpus = [id2word.doc2bow(doc) for doc in doc_ls]  # convert corpus to BoW format\n",
    "\n",
    "tfidf = TfidfModel(corpus)  # fit model\n",
    "vector = tfidf[corpus[0]]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=2, num_nnz=18)\n",
      "==============\n",
      "[(3, 0.8164965809277261), (4, 0.4082482904638631), (5, 0.4082482904638631)]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)\n",
    "print(\"==============\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
